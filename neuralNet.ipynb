{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66531da3",
   "metadata": {},
   "source": [
    "Implementing a Neural Network with negative sampling for word2vec task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67dbe35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"\n",
    "    The main object we're going to use accross this notebook\n",
    "    It's a neural network that takes as input a list of \n",
    "    layers nodes\n",
    "    \n",
    "    Ex: [2, 3, 1] is a 3 layer network, with 2 neurons of input, 3 neurons \n",
    "    in the hidden layer and 1 for the output layer\n",
    "    \n",
    "    Supposedly it can take more than just 3 layers but I didnt test it\n",
    "    \n",
    "    It initializes an object with the proper weights, biases, activations and z\n",
    "    based on the layers list. It also has the layers list and the number of layers\n",
    "    \n",
    "    The weights and biases initialized following a Gaussian with standard deviation 1/sqrt(n_in)\n",
    "    with n_in = number of weights into the neuron\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: list):        \n",
    "        np.random.seed(42)        \n",
    "        b = []\n",
    "        w = []\n",
    "        a = []\n",
    "        z = []\n",
    "        for l in range(0, len(layers)):\n",
    "            # skipping one layer for the weights and biases\n",
    "            if (l+1) < len(layers):\n",
    "                b.append(np.random.normal(loc=0, scale=1,size=layers[l+1]))\n",
    "                wScale = 1/np.sqrt(layers[l])\n",
    "                w.append(np.random.normal(loc=0,scale=wScale,size=[layers[l],layers[l+1]]))\n",
    "                #print(w[l])\n",
    "            a.append(np.zeros(layers[l]))\n",
    "            z.append(np.zeros(layers[l]))\n",
    "        # b[i][j] -> \"i\" is which layer, \"j\" which neuron\n",
    "        # w[i][j][k] -> \"i\" is which layer, \"j\" which neuron of the first layer, \"k\" which neuron of the second layer\n",
    "        self.b = b\n",
    "        self.w = w\n",
    "        self.a = a\n",
    "        self.z = z\n",
    "        self.nLayers = len(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "    @staticmethod\n",
    "    def copy(net):\n",
    "        copiedNet = Network([784,30,10])\n",
    "        copiedNet.a = np.copy(net.a)\n",
    "        copiedNet.z = np.copy(net.z)\n",
    "        for l in range(2):\n",
    "            copiedNet.w[l] = np.copy(net.w[l])\n",
    "            copiedNet.b[l] = np.copy(net.b[l])\n",
    "        return copiedNet\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378ffc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(n: float):\n",
    "    return 1.0/(1.0+np.exp(-n))\n",
    "\n",
    "def sigmoid_derivative(n: float):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(n)*(1-sigmoid(n))\n",
    "\n",
    "def softmax(n: float):\n",
    "    return np.exp(n)/sum(np.exp(n))\n",
    "\n",
    "def cross_entropy_loss(yHat, y):\n",
    "    if y == 1:\n",
    "        return -np.log(yHat)\n",
    "    else:\n",
    "        return -np.log(1 - yHat)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea48a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(net: Network) -> Network:\n",
    "    \"\"\"\n",
    "    Feedforwading the activations to the next layer\n",
    "    \n",
    "    It will take as input the network already with the input image as the activation \n",
    "    on the first layer and then feedforward to the next layrse\n",
    "    \n",
    "    It returns the network with all the activations set\n",
    "    \"\"\"\n",
    "    \n",
    "    for l in range(0, net.nLayers-1):\n",
    "        net.z[l+1] = np.dot(np.transpose(net.a[l]), net.w[l]) + net.b[l]\n",
    "    \n",
    "        # if its the last layer it will apply softmax\n",
    "        if l == net.nLayers-2:\n",
    "            net.a[l+1] = softmax(net.z[l+1])\n",
    "        else:\n",
    "            net.a[l+1] = sigmoid(net.z[l+1])\n",
    "\n",
    "            \n",
    "    return net\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10120406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setInput(net: Network, oneHotTokens):\n",
    "    \"\"\"\n",
    "    Inputs the MNIST number into the network, since the number is a 28x28 matrix, \n",
    "    we transform it into a 784 array\n",
    "    \n",
    "    We also scale the pixels as to be between 0 and 1 for the sigmoid function \n",
    "    instead of 0 and 255\n",
    "    \n",
    "    Returns the network with the proper activations on all layers since it pass \n",
    "    through the feedforward step\n",
    "    \"\"\"\n",
    "    net.z[0] = np.zeros(len(oneHotTokens))\n",
    "    net.a[0] = np.zeros(len(oneHotTokens))\n",
    "    index = np.argmax(oneHotTokens)\n",
    "    net.z[0][index] = 1\n",
    "    net.a[0][index] = 1\n",
    "    net = feedForward(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d026ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNetwork(net: Network, test_X, test_y, nTests: int):\n",
    "    \"\"\"\n",
    "    A function to test our network\n",
    "    \n",
    "    It returns the overall accuracy and the numbers our network guessed\n",
    "    \"\"\"\n",
    "    \n",
    "    correctOutput = 0\n",
    "    X = test_X[:nTests]\n",
    "    y = test_y[:nTests]\n",
    "    outputs = np.zeros((net.layers[-1]))\n",
    "    error = 0\n",
    "    vocabSize = net.layers[-1]\n",
    "    for i in range(nTests):\n",
    "        net = setInput(net, X[i])\n",
    "        networkOutput = np.argmax(net.a[-1])\n",
    "        outputs[networkOutput] += 1\n",
    "        #print(f\"number: {y[i]}, networkOutput: {networkOutput}, activations: {net.a[-1]}\")\n",
    "        error += pow((y[i] - net.a[-1]), 2).mean()\n",
    "    return error, outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491dbfe",
   "metadata": {},
   "source": [
    "The list below is all equations that were used to compute the erros and then propagate through the network:\n",
    "\n",
    "To calculate the error on the last layer: \n",
    "$$\\delta^L = (a^L - y)\\odot \\sigma'(z^L)$$\n",
    "\n",
    "To calculate the error on the other layers:\n",
    "$$\\delta^l = ((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l)$$\n",
    "\n",
    "To repass the error to the bias: \n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$$\n",
    "\n",
    "To repass the error to the weights:\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k\\delta^l_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b397ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(net: Network, y, nNegSamples, unigramTable, SIdic) -> Network:\n",
    "    \"\"\"\n",
    "    The backpropagation step: first we calculate the error on the last layer, \n",
    "    then we pass to the previous layers all the while applying the error \n",
    "    to the weights and biases. Here we used Cross-entropy as our cost function\n",
    "    \n",
    "    Example on a 3 layer network: We calculate the error on the last layer, \n",
    "    apply it to the last layer's weights and biases, and then calculate the \n",
    "    error on the next layer, propagate to the weights and biases and it's done\n",
    "    \n",
    "    It takes as input the network and the label of the number the network was activated on\n",
    "    \n",
    "    It returns the modifications to the weights and biases (nablaW and nablaB) \n",
    "    the network should have\n",
    "    \"\"\"\n",
    "    layers = net.layers\n",
    "    nablaB = [np.zeros(i.shape) for i in net.b]\n",
    "    nablaW = [np.zeros(i.shape) for i in net.w]\n",
    "    delta = np.array(net.a[-1]) - 0\n",
    "    \n",
    "    \n",
    "    # findig n random words from the table\n",
    "    randomWords = np.random.randint(len(unigramTable), size=nNegSamples)\n",
    "    negSamples = []\n",
    "    for i in randomWords:\n",
    "        # finding the int representation of the word and the position on the oneHotVec\n",
    "        negSamples.append(SIdic[unigramTable[i]])\n",
    "    \n",
    "    # finding the correct word on the oneHotVector y\n",
    "    correctWord = np.argmax(y)\n",
    "\n",
    "    \n",
    "    # if the correct word was chosen, it removes from the negative samples\n",
    "    if correctWord in negSamples:\n",
    "        negSamples = np.delete(negSamples, np.where(negSamples == correctWord))\n",
    "        \n",
    "    delta[correctWord] += (net.a[-1][correctWord] - 1)    \n",
    "    for sample in negSamples:\n",
    "        delta[sample] += (net.a[-1][sample] - 0)\n",
    "    \n",
    "    inputWord = np.argmax(net.z[0])\n",
    "    for l in range(net.nLayers-1, 0, -1):\n",
    "        #nablaB and nablaW have -1 because they only have 2 layers instead of 3\n",
    "        nablaB[l-1] = delta\n",
    "                \n",
    "  \n",
    "        nablaW[l-1] = np.outer(net.a[l-1], delta) \n",
    "        \n",
    "        # finding the error one layer behind\n",
    "        # in the book it needs a transpose because its weight[layer][receivingNeuron][givingNeuron]\n",
    "        # but my implementation uses weight[layer][givingNeuron][receivingNeuron] so it's not necessary\n",
    "        delta = (np.dot(net.w[l-1], delta))*sigmoid_derivative(net.z[l-1])\n",
    "        \n",
    "    return nablaB, nablaW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8536b0c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(net: Network, X: list, y: list, nNegSamples, unigramTable, SIdic, batchSize: int, nEpochs: int, \n",
    "        learningRate, lamb) -> Network:\n",
    "    \"\"\"\n",
    "    Implementation of Stochastic Gradient Descent\n",
    "    \n",
    "    It takes as input the network, the MNIST dataset, the MNIST labels of the dataset, \n",
    "    the size of the batch to do gradient descent, the number of epochs it should run,\n",
    "    the learning rate eta (I found the best eta to be in the order of 1s)\n",
    "    and the regularization term lambda\n",
    "    \n",
    "    It returns a trained network\n",
    "    \"\"\"\n",
    "    bestAcc = 0\n",
    "    bestEpoch = 0\n",
    "    minError = np.inf\n",
    "    eta = learningRate\n",
    "    for epoch in range(nEpochs):\n",
    "        batch = rd.sample(range(len(X)), batchSize)\n",
    "        nablaB = [np.zeros(i.shape) for i in net.b]\n",
    "        nablaW = [np.zeros(i.shape) for i in net.w]\n",
    "        for i in batch:\n",
    "            net = setInput(net, X[i])\n",
    "            # finding what should be modified based on this particular example\n",
    "            deltaNablaB, deltaNablaW = backProp(net, y[i], nNegSamples, unigramTable, SIdic)\n",
    "            # passing this modifications to our overall modifications matrices\n",
    "            for l in range(net.nLayers-1):\n",
    "                nablaB[l] += deltaNablaB[l]\n",
    "                nablaW[l] += deltaNablaW[l]\n",
    "        \n",
    "        # applying the changes to our network\n",
    "        for l in range(net.nLayers-1):\n",
    "            net.b[l] = net.b[l] - eta * (nablaB[l]/batchSize) \n",
    "            net.w[l] = net.w[l] - eta * (nablaW[l]/batchSize) - eta * (lamb/batchSize) *  net.w[l]\n",
    "        error, outputs = testNetwork(net, X, y, nTests=batchSize)\n",
    "        if error < minError:\n",
    "            minError = error\n",
    "            bestEpoch = epoch\n",
    "        #print(f'learningRate: {learningRate} epochs: {epoch} error: {error}, outputs: {outputs}')\n",
    "        if epoch%batchSize == 0:\n",
    "            print(f'learningRate: {learningRate} epochs: {epoch} error: {error}')\n",
    "    print(f'min error: {minError} on epoch: {bestEpoch}')\n",
    "    return net\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d07d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
