{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f98bdc96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# dataset imported from\n",
    "csv_location = r''\n",
    "twitters = pd.read_csv(csv_location, usecols=['text'])\n",
    "twitters\n",
    "\n",
    "vocabSize = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "65f18631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts, vocabSize):\n",
    "    ppt = []\n",
    "\n",
    "    for text in texts:\n",
    "        # removing everything after @\n",
    "        filteredText = re.sub(r'@\\S+', \"\", text);\n",
    "        # removing URLS\n",
    "        filteredText = re.sub(r'http\\S+', '', filteredText)\n",
    "        filteredText = re.sub(r'http\\S+', '', filteredText)\n",
    "        # removing everything that is not a letter or space\n",
    "        filteredText = re.sub(r'[^\\w ]', u'', filteredText)\n",
    "        filteredText = re.sub(r'[^a-zA-Z\\s]', u'', filteredText, flags=re.UNICODE)\n",
    "        # removing trailing whitespaces and \\n\n",
    "        filteredText = re.sub(' +', ' ', filteredText)\n",
    "        filteredText = filteredText.strip()\n",
    "        filteredText = filteredText.replace('\\n', '')\n",
    "        # lower casing\n",
    "        ppt.append(filteredText.lower())\n",
    "\n",
    "    # keeping only the first vocabSize words of the text\n",
    "    # first finding the vocabSize first elements\n",
    "    t = ' '.join(ppt)\n",
    "    t = t.split(' ')\n",
    "    d = dict()\n",
    "    for word in t:\n",
    "        d[word] = 0\n",
    "    for word in t:\n",
    "        d[word] += 1\n",
    "    wordCount = dict(sorted(d.items(), key=lambda item: item[1], reverse=True))\n",
    "    # making a dictionary of words to keep or to discard (discarded words will be 0 in the dic)\n",
    "    wordsKept = dict()\n",
    "    i = 0\n",
    "    for word in wordCount:\n",
    "        if i < vocabSize:\n",
    "            wordsKept[word] = word\n",
    "        else:\n",
    "            wordsKept[word] = '0'\n",
    "        i += 1\n",
    "    vocabText = []\n",
    "    for sentence in ppt:\n",
    "        t = sentence.split(' ')\n",
    "        s = []\n",
    "        for word in t:\n",
    "            s.append(wordsKept[word])\n",
    "        vocabText.append(' '.join(s))\n",
    "    # creating a new dict that keeps only vocabSize words\n",
    "    i = 0\n",
    "    smallWordCount = dict()\n",
    "    for word, count in wordCount.items():\n",
    "        smallWordCount[word] = count\n",
    "        i += 1\n",
    "        if i >= vocabSize:\n",
    "            break\n",
    "    # returning the preprocessed text with only the first vocabSize most frequent words\n",
    "    # returning the count of that vocabulary\n",
    "    return vocabText, smallWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "63cd0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2int(texts):\n",
    "    t = ' '.join(texts)\n",
    "    t = t.split(' ')\n",
    "    d = dict()\n",
    "    d = dict([(y,x-1) for x,y in enumerate(sorted(set(t)))])\n",
    "    # assigning '0' as the last position as to not cause conflict with the neural net later\n",
    "    d['0'] = len(d) - 1\n",
    "    intTexts = []\n",
    "    for sentence in texts:\n",
    "        t = []\n",
    "        for word in sentence.split(' '):\n",
    "            t.append(d[word])\n",
    "        intTexts.append(t)\n",
    "    dS = {v: k for k, v in d.items()}\n",
    "    return intTexts, d, dS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "55217202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shouldKeepWord(word, sample, wordCount, corpusSize):\n",
    "    # if its a word out of the vocabulary\n",
    "    if word == '0':\n",
    "        return False\n",
    "    \n",
    "    n = wordCount[word]\n",
    "    z = n/corpusSize\n",
    "    p = (np.sqrt(z/sample) + 1) * sample/z\n",
    "    \n",
    "    r = np.random.uniform(0,1,1)\n",
    "    if p > r:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(processedText, vocabSize, wordCount, ISdic, windowSize=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    # the position in the oneHotToken of a word will be it's integer representation\n",
    "    # for example: word \"table\" -> SIdic['table'] = 15 -> oneHotTokens[15] represent the oneHotToken of table\n",
    "    oneHotTokens = np.zeros((vocabSize, vocabSize))\n",
    "    np.fill_diagonal(oneHotTokens, 1)\n",
    "    \n",
    "    corpusSize = 0\n",
    "    for word, count in wordCount.items():\n",
    "        corpusSize += count\n",
    "    for sentence in processedText:\n",
    "        s = []\n",
    "        \n",
    "        # implementing subsampling of frequent words\n",
    "        for word in sentence:\n",
    "            if shouldKeepWord(ISdic[word], 0.001, wordCount, corpusSize):\n",
    "                s.append(word)\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            centerWord = s[i]\n",
    "            if centerWord == vocabSize:\n",
    "                continue\n",
    "            j = i\n",
    "            while j < i + windowSize and j < len(sentence)-1:\n",
    "                j += 1\n",
    "                if sentence[j] == vocabSize:\n",
    "                    continue\n",
    "                X.append(oneHotTokens[centerWord])\n",
    "                y.append(oneHotTokens[sentence[j]])\n",
    "            j = i\n",
    "            while j > i - windowSize and j >= 1:\n",
    "                j -= 1\n",
    "                if sentence[j] == vocabSize:\n",
    "                    continue\n",
    "                X.append(oneHotTokens[centerWord])\n",
    "                y.append(oneHotTokens[sentence[j]])\n",
    "        if len(X) > 100000:\n",
    "            break\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d31a5",
   "metadata": {},
   "source": [
    "Formula for table: $P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum \\limits _{j=0}^{n}(f(w_j)^{3/4})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "237e19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTableForNegSampling(wordCount, tableSize=100000000):\n",
    "    # creating a 100M size table \n",
    "    table = np.array(tableSize*['0'], dtype=object)\n",
    "    \n",
    "    probabilities = dict()\n",
    "    powSum = 0\n",
    "    for word, count in wordCount.items():\n",
    "        powSum += pow(count, 3/4)\n",
    "        \n",
    "    # calculating the probabilities for each word\n",
    "    for word, count in wordCount.items():\n",
    "        probabilities[word] = pow(count, 3/4)/ powSum\n",
    "    currentIndex = 0\n",
    "    for word, prob in probabilities.items():\n",
    "        nIndexes = int(round(prob * tableSize, 2))\n",
    "        for i in range(currentIndex, currentIndex + nIndexes):\n",
    "            table[i] = word\n",
    "        \n",
    "        currentIndex += nIndexes\n",
    "    \n",
    "    return table[:currentIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8535cde3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-b9ef4a9d727d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mppt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordCount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwitters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-138-57bc0deba4cb>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(texts, vocabSize)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mfilteredText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'http\\S+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilteredText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# removing everything that is not a letter or space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mfilteredText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^\\w ]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilteredText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mfilteredText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^a-zA-Z\\s]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilteredText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNICODE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# removing trailing whitespaces and \\n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppt, wordCount = preprocess(twitters['text'], vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e3c871d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating the String to Int and Int to String dictionarys\n",
    "intTweets, SIdic, ISdic = words2int(ppt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7ae307a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = getTrainingData(intTweets, vocabSize, wordCount, ISdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "314c4228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = createTableForNegSampling(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "de731e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run neuralNet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "add4bfb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learningRate: 0.1 epochs: 0 error: 1.0014028116375011\n",
      "learningRate: 0.1 epochs: 1000 error: 0.9746208042432293\n",
      "learningRate: 0.1 epochs: 2000 error: 0.9746710971635686\n",
      "learningRate: 0.1 epochs: 3000 error: 0.9743738500364632\n",
      "learningRate: 0.1 epochs: 4000 error: 0.9742430125472216\n",
      "learningRate: 0.1 epochs: 5000 error: 0.9743919097601972\n",
      "learningRate: 0.1 epochs: 6000 error: 0.9743295651572804\n",
      "learningRate: 0.1 epochs: 7000 error: 0.974274993639156\n",
      "learningRate: 0.1 epochs: 8000 error: 0.9741443600852878\n",
      "learningRate: 0.1 epochs: 9000 error: 0.9744101971686799\n",
      "min error: 0.9739590537171726 on epoch: 9049\n"
     ]
    }
   ],
   "source": [
    "# the network has negative sampling\n",
    "net = Network([vocabSize, 10, vocabSize])\n",
    "batchSize = 1000\n",
    "nEpochs = len(X)//batchSize\n",
    "net = SGD(net, X, y, nNegSamples=5, unigramTable=table, SIdic=SIdic, batchSize=batchSize, nEpochs=nEpochs, learningRate=0.1, lamb=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneWordVec(net: Network, word: str, SIdic) -> np.array:\n",
    "    w = SIdic[word]\n",
    "    oneHotTokens = np.zeros(np.shape(net.z[0])[0])\n",
    "    oneHotTokens[w] = 1\n",
    "    net = setInput(net, oneHotTokens)\n",
    "    return net.w[0][w]\n",
    "\n",
    "def getWordVec(net: Network) -> np.array:\n",
    "    w = SIdic[word]\n",
    "    oneHotTokens = np.zeros(np.shape(net.z[0])[0])\n",
    "    oneHotTokens[w] = 1\n",
    "    net = setInput(net, oneHotTokens)\n",
    "    return net.w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0151360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.070181   0.11089551 0.12523967 0.06543846 0.07075161 0.20877218\n",
      " 0.18762408 0.04211152 0.07289377 0.15005178]\n",
      "[0.1575588  0.07987772 0.13130389 0.11445485 0.06520042 0.21607862\n",
      " 0.12691169 0.03123916 0.10374024 0.13316495]\n"
     ]
    }
   ],
   "source": [
    "wv1 = getWordVec(net, 'thrones', SIdic)\n",
    "wv2 = getWordVec(net, 'game', SIdic)\n",
    "print(wv1)\n",
    "print(wv2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
